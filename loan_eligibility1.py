# -*- coding: utf-8 -*-
"""loan_eligibility1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bt6c28kpcqKYpuDEIIYcQJKzQ5L2mRPa
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn import metrics
from sklearn.svm import SVC
from imblearn.over_sampling import RandomOverSampler

import warnings
warnings.filterwarnings('ignore')

#loading dataset
df = pd.read_csv('/loan_data.csv')
df.head()

df.shape #To see the shape of the dataset

df.info() #To print the information of the dataset

#to get values like the mean, count and min of the column
df.describe()

"""**Exploratory Data Analysis**"""

# Exploratory Data Analysis
temp = df['Loan_Status'].value_counts()
plt.pie(temp.values,
        labels=temp.index,
        autopct='%1.1f%%')
plt.show()

"""Here we have an imbalanced dataset. We will have to balance it before training any model on this data."""

plt.subplots(figsize=(15, 5))
for i, col in enumerate(['Gender', 'Married']):
    plt.subplot(1, 2, i+1)
    sb.countplot(data=df, x=col, hue='Loan_Status')
plt.tight_layout()
plt.show()

"""One of the main observations we can draw here is that the chances of getting a loan approved for married people are quite low compared to those who are not married."""

#To find out the outliers in the columns, we can use boxplot.



plt.subplots(figsize=(15, 5))
for i, col in enumerate(['ApplicantIncome', 'LoanAmount']):
    plt.subplot(1, 2, i+1)
    sb.boxplot(df[col])
plt.tight_layout()
plt.show()

#There are some extreme outlier’s in the data we need to remove them.

df = df[df['ApplicantIncome'] < 25000]
df = df[df['LoanAmount'] < 400000]

#Let’s see the mean amount of the loan granted to males as well as females. For that, we will use groupyby() method.

df.groupby('Gender').mean(numeric_only=True)['LoanAmount']

df.groupby(['Married', 'Gender']).mean(numeric_only=True)['LoanAmount']

# Function to apply label encoding
def encode_labels(data):
    for col in data.columns:
        if data[col].dtype == 'object':
            le = LabelEncoder()
            data[col] = le.fit_transform(data[col])

    return data

# Applying function in whole column
df = encode_labels(df)

# Generating Heatmap
sb.heatmap(df.corr() > 0.8, annot=True, cbar=False)
plt.show()

"""**Data Preprocessing**"""

#splitTING the data for training and testing
features = df.drop('Loan_Status', axis=1)
target = df['Loan_Status'].values

X_train, X_val,    Y_train, Y_val = train_test_split(features, target,
                                    test_size=0.2,
                                    random_state=10)

# As the data was highly imbalanced we will balance
# it by adding repetitive rows of minority class.
ros = RandomOverSampler(sampling_strategy='minority',
                        random_state=0)
X, Y = ros.fit_resample(X_train, Y_train)

X_train.shape, X.shape

# Normalizing the features for stable and fast training.
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_val = scaler.transform(X_val)

"""**Model Development**"""

# we are using Support Vector Classifier for training the model
from sklearn.metrics import roc_auc_score
model = SVC(kernel='rbf')
model.fit(X, Y)

print('Training Accuracy : ', metrics.roc_auc_score(Y, model.predict(X)))
print('Validation Accuracy : ', metrics.roc_auc_score(Y_val, model.predict(X_val)))
print()

"""**Model Evaluation**"""

#The confusion matrix is built for the validation data by using the confusion_matrix function from sklearn.metrics
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
training_roc_auc = roc_auc_score(Y, model.predict(X))
validation_roc_auc = roc_auc_score(Y_val, model.predict(X_val))
print('Training ROC AUC Score:', training_roc_auc)
print('Validation ROC AUC Score:', validation_roc_auc)
print()
cm = confusion_matrix(Y_val, model.predict(X_val))

#roc_auc_score function is used to calculate the ROC-AUC score for both the training and validation datasets.
#This allows us to evaluate the performance of the model on both datasets and compare the results.

#we plot the confusion matrix using the plot_confusion_matrix function from the sklearn.metrics.plot_confusion_matrix

plt.figure(figsize=(6, 6))
sb.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

#The heatmap visualization of the confusion matrix provides a quick and easy way to understand the performance of the model.

from sklearn.metrics import classification_report
print(classification_report(Y_val, model.predict(X_val)))

#The classification_report function is used to:
# Evaluate model performance: Get a comprehensive report on the performance of a classification model.